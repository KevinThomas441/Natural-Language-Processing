{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial Setup:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Links to word vector downloads:<br>\n",
    "Glove: http://nlp.stanford.edu/data/glove.6B.zip <br>\n",
    "File used: glove.6B.50d.txt<br>\n",
    "\n",
    "LexVec: https://www.dropbox.com/s/kguufyc2xcdi8yk/lexvec.enwiki%2Bnewscrawl.300d.W.pos.vectors.gz?dl=1 <br>\n",
    "\n",
    "aclmb folder, glove.6B.50d.txt and lexvec.enwiki+newscrawl.300d.W.pos.vectors must be placed in the same directory as the notebook.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import relevant libraries\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 1:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need functions to assess and evaluate the performance of our models. We will implement those first.\n",
    "Create one function to calculate precision, one function to calculate recall and\n",
    "one function to calculate f-measure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Confusion Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we must calculate a confusion matrix. A confusion matrix is used to describe the performance of a classification model. From the confusion matrix, we can calculate the values for True Positive, False Positive, False Negative and True Negative.<br>\n",
    "Next, we use the values calculated above to calculate precision, recall and F1 score (f-measure)<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to calculate a confusion matrix\n",
    "def confusion_matrix(y_pred, y_test):\n",
    "    c_mat = np.zeros((2, 2))\n",
    "    for p, t in zip(y_pred, y_test):\n",
    "        c_mat[p][t] += 1\n",
    "    return c_mat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Precision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Precision is calculated as:<br>\n",
    "Precision = True Positive / (True Positive + False Positive)<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to calculate precision\n",
    "def calc_precision(y_pred, y_test):\n",
    "    cm = confusion_matrix(y_pred, y_test)\n",
    "    \n",
    "    #Precision = True Positive / (True Positive + False Positive)\n",
    "    return (cm[1, 1] / (cm[1, 1] + cm[1, 0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Recall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall is calculated as:<br>\n",
    "Recall = True Positive / (True Positive + False Negative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate recall\n",
    "def calc_recall(y_pred, y_test):\n",
    "    cm = confusion_matrix(y_pred, y_test)\n",
    "    \n",
    "    #Recall = True Positive / (True Positive + False Negative)\n",
    "    return (cm[1, 1] / (cm[1, 1] +  cm[0, 1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### F1 Score/F-measure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "F1 score is a combination of precision and recall. F1 score is calculated as:<br>\n",
    "F1 Score = 2 * (Precision * Recall) / (Precision + Recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Function to calculate F1 Score\n",
    "def calc_fmeasure(y_pred, y_test):\n",
    "    cm = confusion_matrix(y_pred, y_test)\n",
    "    \n",
    "    #Precision = True Positive / (True Positive + False Positive)\n",
    "    precision = (cm[1, 1] / (cm[1, 1] + cm[1, 0]))\n",
    "    \n",
    "    #Recall = True Positive / (True Positive + False Negative)\n",
    "    recall = (cm[1, 1] / (cm[1, 1] + cm[0, 1]))\n",
    "    \n",
    "    #F1 Score = 2 * (Precision * Recall) / (Precision + Recall)\n",
    "    return (2 * precision * recall) / (precision + recall)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additional function to calculate accuracy. Used in Problem 7."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to calculate accuracy\n",
    "def calc_accuracy(y_pred, y_test):\n",
    "    cm = confusion_matrix(y_pred, y_test)\n",
    "    accuracy = (cm[0, 0]+cm[1, 1])/(cm[0, 0]+cm[1, 1]+cm[0, 1]+cm[1, 0])\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Majority Class Baseline. We will create majority class\n",
    "baseline to evaluate our initial model performance – which is the simplest\n",
    "baseline. The label for the test data should be the majority class found in\n",
    "training data.\n",
    "You should report P, R and F-score (using the functions you wrote to solve\n",
    "Problem 1) for both training and test data to obtain full marks on this problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To create a majority class baseline, we need to figure out which class is the majority class, i.e. what class does a majority of the data belong to. <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import the load_files method from sklearn.datasets to import the data\n",
    "from sklearn.datasets import load_files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the load_files function from sklearn.datasets to load the text files. This is done because load_files allows us the label files by the folders they are stored in. This is ideal, as in our case, the positive and negative examples are stored in separate subfolders. Negative reviews are encoded with the label 0 and positive reviews are labelled with 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: the following two commands occasionally take a long time to execute. Rerunning the commands a couple of times usually helps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import the training data\n",
    "directory = r'aclImdb/train/'\n",
    "train = load_files(directory, categories = ['neg', 'pos'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import the testing data\n",
    "directory = r'aclImdb/test/'\n",
    "test = load_files(directory, categories = ['neg', 'pos'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We count the number of reviews belonging to each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Positive Reviews in Training Data:  12500\n",
      "Number of Negative Reviews in Training Data:  12500\n"
     ]
    }
   ],
   "source": [
    "#Printing out the number of reviews of each class in the training data\n",
    "print(\"Number of Positive Reviews in Training Data: \", (train.target==1).sum())\n",
    "print(\"Number of Negative Reviews in Training Data: \", (train.target==0).sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can see that the number of reviews per class in the training data is the same (12500). Therefore, we will compute our metrics for both cases.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Majority Class: Negative"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We consider 0 (Negative) to be our majority class label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Assigning a predicted value of 0 to all reviews\n",
    "y_pred = [0]*25000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics for Majority Class Baseline (Training Data):\n",
      "Precision:  nan\n",
      "Recall:  0.0\n",
      "F1 Score:  nan\n"
     ]
    }
   ],
   "source": [
    "#Calculating metrics\n",
    "print(\"Metrics for Majority Class Baseline (Training Data):\")\n",
    "print(\"Precision: \", calc_precision(y_pred, train.target)*100)\n",
    "print(\"Recall: \", calc_recall(y_pred, train.target)*100)\n",
    "print(\"F1 Score: \", calc_fmeasure(y_pred, train.target)*100)\n",
    "# print(\"Accuracy: \", calc_accuracy(y_pred, train.target)*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics for Majority Class Baseline (Test Data):\n",
      "Precision:  nan\n",
      "Recall:  0.0\n",
      "F1 Score:  nan\n"
     ]
    }
   ],
   "source": [
    "#Calculating metrics\n",
    "print(\"Metrics for Majority Class Baseline (Test Data):\")\n",
    "print(\"Precision: \", calc_precision(y_pred, test.target)*100)\n",
    "print(\"Recall: \", calc_recall(y_pred, test.target)*100)\n",
    "print(\"F1 Score: \", calc_fmeasure(y_pred, test.target)*100)\n",
    "# print(\"Accuracy: \", calc_accuracy(y_pred, test.target)*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Majority Class: Positive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we consider 1 (Positive) to be our majority class label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Assigning a predicted value of 1 to all reviews\n",
    "y_pred = [1]*25000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics for Majority Class Baseline (Training Data):\n",
      "Precision:  50.0\n",
      "Recall:  100.0\n",
      "F1 Score:  66.66666666666666\n"
     ]
    }
   ],
   "source": [
    "#Calculating metrics\n",
    "print(\"Metrics for Majority Class Baseline (Training Data):\")\n",
    "print(\"Precision: \", calc_precision(y_pred, train.target)*100)\n",
    "print(\"Recall: \", calc_recall(y_pred, train.target)*100)\n",
    "print(\"F1 Score: \", calc_fmeasure(y_pred, train.target)*100)\n",
    "# print(\"Accuracy: \", calc_accuracy(y_pred, train.target)*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics for Majority Class Baseline:\n",
      "Precision:  50.0\n",
      "Recall:  100.0\n",
      "F1 Score:  66.66666666666666\n"
     ]
    }
   ],
   "source": [
    "#Calculating metrics\n",
    "print(\"Metrics for Majority Class Baseline:\")\n",
    "print(\"Precision: \", calc_precision(y_pred, test.target)*100)\n",
    "print(\"Recall: \", calc_recall(y_pred, test.target)*100)\n",
    "print(\"F1 Score: \", calc_fmeasure(y_pred, test.target)*100)\n",
    "# print(\"Accuracy: \", calc_accuracy(y_pred, test.target)*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Review Length Baseline. We will create baseline to\n",
    "evaluate our model performance – which takes into account length of the\n",
    "review.\n",
    "For this baseline, you should try setting various thresholds of review length to\n",
    "classify them as positive or negative. For example, all reviews > 50 words in\n",
    "length can be classified as positive. You should experiment with at least 3\n",
    "different thresholds and document your reasons why you chose these\n",
    "thresholds.\n",
    "For each threshold, you should report P, R and F-score (using the functions you\n",
    "wrote to solve Problem 1) for both training and test data to obtain full marks on\n",
    "this problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to calculate the length of each review in our dataset.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['length'] = [len(review) for review in train.data]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we see what the maximum and minimum values for the length are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum Length:  13704\n",
      "Minimum Length:  52\n"
     ]
    }
   ],
   "source": [
    "print(\"Maximum Length: \", max(train.length))\n",
    "print(\"Minimum Length: \", min(train.length))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our experiments, we create a threshold based on various percentile values of review length. Reviews above this threshold are given a positive sentiment and the reviews below the threshold are given a negative sentiment. The threshold is positioned at the 25th, 50th and 75th percentile values of review length.<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training Data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.DataFrame.from_dict({key: train[key] for key in train.keys()\n",
    "                               & {'data', 'filenames', 'target', 'length'}} )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setting the threshold to the 25th Percentile\n",
    "train_data['y_pred'] = train_data.length.apply(lambda x: 1 if x > np.percentile(train.length, 25) else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics for Review Length Baseline (25th Percentile):\n",
      "Precision:  49.34877762357212\n",
      "Recall:  73.96000000000001\n",
      "F1 Score:  59.19830953448165\n"
     ]
    }
   ],
   "source": [
    "#Calculating metrics\n",
    "print(\"Metrics for Review Length Baseline (25th Percentile):\")\n",
    "print(\"Precision: \", calc_precision(train_data['y_pred'], train.target)*100)\n",
    "print(\"Recall: \", calc_recall(train_data['y_pred'], train.target)*100)\n",
    "print(\"F1 Score: \", calc_fmeasure(train_data['y_pred'], train.target)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setting the threshold to the 50th Percentile\n",
    "train_data['y_pred'] = train_data.length.apply(lambda x: 1 if x > np.percentile(train.length, 50) else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics for Review Length Baseline (50th Percentile):\n",
      "Precision:  50.19617263191609\n",
      "Recall:  50.151999999999994\n",
      "F1 Score:  50.17407659370122\n"
     ]
    }
   ],
   "source": [
    "#Calculating metrics\n",
    "print(\"Metrics for Review Length Baseline (50th Percentile):\")\n",
    "print(\"Precision: \", calc_precision(train_data['y_pred'], train.target)*100)\n",
    "print(\"Recall: \", calc_recall(train_data['y_pred'], train.target)*100)\n",
    "print(\"F1 Score: \", calc_fmeasure(train_data['y_pred'], train.target)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setting the threshold to the 75th percentile\n",
    "train_data['y_pred'] = train_data.length.apply(lambda x: 1 if x > np.percentile(train.length, 75) else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics for Review Length Baseline (75th Percentile):\n",
      "Precision:  51.896303408545364\n",
      "Recall:  25.944\n",
      "F1 Score:  34.59384500506694\n"
     ]
    }
   ],
   "source": [
    "#Calculating metrics\n",
    "print(\"Metrics for Review Length Baseline (75th Percentile):\")\n",
    "print(\"Precision: \", calc_precision(train_data['y_pred'], train.target)*100)\n",
    "print(\"Recall: \", calc_recall(train_data['y_pred'], train.target)*100)\n",
    "print(\"F1 Score: \", calc_fmeasure(train_data['y_pred'], train.target)*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test Data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['length'] = [len(review) for review in test.data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = pd.DataFrame.from_dict({key: test[key] for key in test.keys()\n",
    "                               & {'data', 'filenames', 'target', 'length'}} )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setting the threshold to the 25th Percentile\n",
    "test_data['y_pred'] = test_data.length.apply(lambda x: 1 if x > np.percentile(test.length, 25) else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics for Review Length Baseline (25th Percentile):\n",
      "Precision:  49.28457020822211\n",
      "Recall:  73.848\n",
      "F1 Score:  59.116234390009616\n"
     ]
    }
   ],
   "source": [
    "#Calculating metrics\n",
    "print(\"Metrics for Review Length Baseline (25th Percentile):\")\n",
    "print(\"Precision: \", calc_precision(test_data['y_pred'], test.target)*100)\n",
    "print(\"Recall: \", calc_recall(test_data['y_pred'], test.target)*100)\n",
    "print(\"F1 Score: \", calc_fmeasure(test_data['y_pred'], test.target)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setting the threshold to the 50th Percentile\n",
    "test_data['y_pred'] = test_data.length.apply(lambda x: 1 if x > np.percentile(test.length, 50) else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics for Review Length Baseline (50th Percentile):\n",
      "Precision:  49.47933354694008\n",
      "Recall:  49.416\n",
      "F1 Score:  49.447646493756004\n"
     ]
    }
   ],
   "source": [
    "#Calculating metrics\n",
    "print(\"Metrics for Review Length Baseline (50th Percentile):\")\n",
    "print(\"Precision: \", calc_precision(test_data['y_pred'], test.target)*100)\n",
    "print(\"Recall: \", calc_recall(test_data['y_pred'], test.target)*100)\n",
    "print(\"F1 Score: \", calc_fmeasure(test_data['y_pred'], test.target)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setting the threshold to the 75th Percentile\n",
    "test_data['y_pred'] = test_data.length.apply(lambda x: 1 if x > np.percentile(test.length, 75) else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics for Review Length Baseline (75th Percentile):\n",
      "Precision:  50.24023062139654\n",
      "Recall:  25.096\n",
      "F1 Score:  33.472044387537345\n"
     ]
    }
   ],
   "source": [
    "#Calculating metrics\n",
    "print(\"Metrics for Review Length Baseline (75th Percentile):\")\n",
    "print(\"Precision: \", calc_precision(test_data['y_pred'], test.target)*100)\n",
    "print(\"Recall: \", calc_recall(test_data['y_pred'], test.target)*100)\n",
    "print(\"F1 Score: \", calc_fmeasure(test_data['y_pred'], test.target)*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above experiments, we can see that shorter reviews are more likely to be positive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementing the Naïve Bayes classifier. You can use the\n",
    "built-in Naive Bayes model from sklearn to train a classifier. Here is the\n",
    "documentation for how to implement GaussianNB (discussed in class) using\n",
    "sklearn.\n",
    "You will train your classifier on the words contained in the positive and negative\n",
    "reviews, based on the algorithm discussed in class.\n",
    "You should report P, R and F-score (using the functions you wrote to solve\n",
    "Problem 1) for both training and test data to obtain full marks on this problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing relevant libraries\n",
    "from nltk import word_tokenize\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.naive_bayes import GaussianNB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To input text data into a machine learning model, the text data must be converted to a numerical representation. In this case, we use CountVectorizer from sklearn.feature_extraction to create a vector of token count vectors for each document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating bag of words of the documents using the fit_transform method of the CountVectorizer class\n",
    "word_vec = CountVectorizer(min_df=2, tokenizer=word_tokenize)         \n",
    "word_counts = word_vec.fit_transform(train.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To avoid discrepancies with regard to higher word counts in longer documents, we create a Term Frequency - Inverse Document Frequency vector to normalize word frequencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating TF-IDF vectors of the training data\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "train_tfidf = tfidf_transformer.fit_transform(word_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creaing bag of words vectors and TF-IDF vectors of the testing data\n",
    "test_counts = word_vec.transform(test.data)\n",
    "test_tfidf = tfidf_transformer.transform(test_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that our data is represented appropriately, we can move on to training and testing our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a Gaussian Naive Bayes classifier object\n",
    "GNB = GaussianNB()\n",
    "\n",
    "#The unique classes are computed because the first iteration of partial_fit requires it\n",
    "unique_classes = np.unique(train.target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to memory constraints, the fit method of the GaussianNB class cannot be used. Therefore, we use the partial_fit function from the GaussianNB class to train our model incrementally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GaussianNB(priors=None, var_smoothing=1e-09)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GNB.partial_fit(train_tfidf[0:10000].toarray(), train.target[0:10000], classes=unique_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GaussianNB(priors=None, var_smoothing=1e-09)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GNB.partial_fit(train_tfidf[10000:20000].toarray(), train.target[10000:20000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GaussianNB(priors=None, var_smoothing=1e-09)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GNB.partial_fit(train_tfidf[20000:].toarray(), train.target[20000:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training Data:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the model to predict the labels for the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Predicting the target variables for the training data\n",
    "y_pred = []\n",
    "for i in range(0, 25000):\n",
    "    y_pred.append(int(GNB.predict(train_tfidf[i].toarray())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Storing the training target variables in a list variable y\n",
    "y = train.target.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics for training data using Gaussian Naive Bayes classification\n",
      "Precision:  99.33763136977832\n",
      "Recall:  89.984\n",
      "F1 Score:  94.42975275993787\n"
     ]
    }
   ],
   "source": [
    "#Calculating metrics\n",
    "print(\"Metrics for training data using Gaussian Naive Bayes classification\")\n",
    "print(\"Precision: \", calc_precision(y_pred, y)*100)\n",
    "print(\"Recall: \", calc_recall(y_pred, y)*100)\n",
    "print(\"F1 Score: \", calc_fmeasure(y_pred, y)*100)\n",
    "#print(\"Accuracy: \", calc_accuracy(y_pred, y)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "del y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test Data:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we use the model to predict the labels of the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Predicting the target variables for the test data\n",
    "y_pred = []\n",
    "for i in range(0, 25000):\n",
    "    y_pred.append(int(GNB.predict(test_tfidf[i].toarray())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Storing the test target variables in a list variable y\n",
    "y = test.target.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics for testing data using Gaussian Naive Bayes classification\n",
      "Precision:  62.775149146266195\n",
      "Recall:  48.824\n",
      "F1 Score:  54.92754927549276\n"
     ]
    }
   ],
   "source": [
    "#Calculating metrics\n",
    "print(\"Metrics for testing data using Gaussian Naive Bayes classification\")\n",
    "print(\"Precision: \", calc_precision(y_pred, y)*100)\n",
    "print(\"Recall: \", calc_recall(y_pred, y)*100)\n",
    "print(\"F1 Score: \", calc_fmeasure(y_pred, y)*100)\n",
    "#print(\"Accuracy: \", calc_accuracy(y_pred, y)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Deleting the model and predicted values to free up memory\n",
    "del GNB, y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementing your own classifier. You can implement\n",
    "your own classifier – by using another classifier from sklearn package – SVM or\n",
    "Logistic Regression (for example) or you can experiment with different features\n",
    "and add them to your Naïve Bayes model. As before, you should report P, R and\n",
    "F-score (using the functions you wrote to solve Problem 1) for both training and\n",
    "test data to obtain full marks on this problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The classifier implemented here is a linear Support Vector Machine. The main difference between Naive Bayes and SVM is that Naive Bayes considers each feature to be independant while SVM considers any implicit relationships between the features.<br>\n",
    "We use the SGDClassifier class to implement this instead of the SVC class from sklearn.svm because the former supports minibatch learning using the partial_fit method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "svm_clf = linear_model.SGDClassifier()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Incrementally training the classifier in batches using the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SGDClassifier(alpha=0.0001, average=False, class_weight=None,\n",
       "       early_stopping=False, epsilon=0.1, eta0=0.0, fit_intercept=True,\n",
       "       l1_ratio=0.15, learning_rate='optimal', loss='hinge', max_iter=None,\n",
       "       n_iter=None, n_iter_no_change=5, n_jobs=None, penalty='l2',\n",
       "       power_t=0.5, random_state=None, shuffle=True, tol=None,\n",
       "       validation_fraction=0.1, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm_clf.partial_fit(train_tfidf[0:10000].toarray(), train.target[0:10000], classes=unique_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SGDClassifier(alpha=0.0001, average=False, class_weight=None,\n",
       "       early_stopping=False, epsilon=0.1, eta0=0.0, fit_intercept=True,\n",
       "       l1_ratio=0.15, learning_rate='optimal', loss='hinge', max_iter=None,\n",
       "       n_iter=None, n_iter_no_change=5, n_jobs=None, penalty='l2',\n",
       "       power_t=0.5, random_state=None, shuffle=True, tol=None,\n",
       "       validation_fraction=0.1, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm_clf.partial_fit(train_tfidf[10000:20000].toarray(), train.target[10000:20000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SGDClassifier(alpha=0.0001, average=False, class_weight=None,\n",
       "       early_stopping=False, epsilon=0.1, eta0=0.0, fit_intercept=True,\n",
       "       l1_ratio=0.15, learning_rate='optimal', loss='hinge', max_iter=None,\n",
       "       n_iter=None, n_iter_no_change=5, n_jobs=None, penalty='l2',\n",
       "       power_t=0.5, random_state=None, shuffle=True, tol=None,\n",
       "       validation_fraction=0.1, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm_clf.partial_fit(train_tfidf[20000:].toarray(), train.target[20000:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the trained model to predict the training labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Predicting the target sentiment values for the training data\n",
    "y_pred = []\n",
    "for i in range(0, 25000):\n",
    "    y_pred.append(int(svm_clf.predict(train_tfidf[i].toarray())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Storing the training target variables in a list variable y\n",
    "y = train.target.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics for training data using Linear SVM classification\n",
      "Precision:  69.3671027619367\n",
      "Recall:  99.256\n",
      "F1 Score:  81.66260777989864\n"
     ]
    }
   ],
   "source": [
    "#Calculating metrics\n",
    "print(\"Metrics for training data using Linear SVM classification\")\n",
    "print(\"Precision: \", calc_precision(y_pred, y)*100)\n",
    "print(\"Recall: \", calc_recall(y_pred, y)*100)\n",
    "print(\"F1 Score: \", calc_fmeasure(y_pred, y)*100)\n",
    "#print(\"Accuracy: \", calc_accuracy(y_pred, y)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "del y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the model to predict the labels for the testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Predicting the target variables for the test data\n",
    "y_pred = []\n",
    "for i in range(0, 25000):\n",
    "    y_pred.append(int(svm_clf.predict(test_tfidf[i].toarray())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Storing the test target variables in a list variable y\n",
    "y = test.target.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics for testing data using Linear SVM classification\n",
      "Precision:  67.11956521739131\n",
      "Recall:  98.8\n",
      "F1 Score:  79.93527508090615\n"
     ]
    }
   ],
   "source": [
    "#Calculating metrics\n",
    "print(\"Metrics for testing data using Linear SVM classification\")\n",
    "print(\"Precision: \", calc_precision(y_pred, y)*100)\n",
    "print(\"Recall: \", calc_recall(y_pred, y)*100)\n",
    "print(\"F1 Score: \", calc_fmeasure(y_pred, y)*100)\n",
    "#print(\"Accuracy: \", calc_accuracy(y_pred, y)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Deleting the model and predicted values to free up memory\n",
    "del svm_clf, y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do you observe about the performance of the\n",
    "different models?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first model, based on Majority Class Baseline, worked on the assumption that the labels for the test data should be the label that is most common in the training data. However, the classes in our training data are perfectly balanced, i.e. they are equal in number. Therefore we calculate the metrics using both labels. We see that the the model fares poorly, even giving us NaN values due to the the lack to true positive values in some cases.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our next model classifies reviews based on review length. A threshold is moved between the maximum and minimum lengths of the training data set. The metrics are best when the threshold is set at 25th percentile of the review lengths. This implies that shorter reviews are more likely to be positive. However, the precision, recall and fmeasure scores are not high which implies that the model is not a very good one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first machine learning approach implemented here is a Gaussian Naive Bayes classifier. We convert the data into a bag of words vector and then further into TFIDF vectors. These vectors are then used to predict the target value. The model performs substantially better that the preceeding models on the training data. However it performed poorly on the test data, indicating that the model suffers from overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our final model uses a linear Support Vector Machine to classify our datasets. This is the best model by far, obtaining similar values for precision, recall and F1 score for both training and testing data, indicating that the model has generalized well. Since SVM considers the relationships between the features in a model, we can safely assume that treating word vectors as mutually independant is an incorrect approach."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vector Semantics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the analogy test. Pick any two sets of pretrained\n",
    "embeddings, implement the analogy prediction method described in Equation\n",
    "2, and compare their accuracies on the eight analogy tasks listed above. Make\n",
    "sure to mention the details of your selection in writing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this part of the assignment, we use the gensim library create word vector models using pretrained embeddings<br>\n",
    "https://radimrehurek.com/gensim/index.html<br>\n",
    "<br>\n",
    "Additionally, gensim's most_similar method helps us find the most similar vector to a target vector using cosine similarity.<br>\n",
    "https://tedboy.github.io/nlps/generated/generated/gensim.models.Word2Vec.most_similar.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reading the test data from the text file\n",
    "f = open(r'word-test.v1.txt', encoding='utf-8')\n",
    "text = f.read()\n",
    "f.close()\n",
    "\n",
    "#Preprocessing the data\n",
    "text = text.split(\":\")[1:]\n",
    "text = [doc.split('\\n') for doc in text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Splitting the test into analogy tests. Keeping only those tests that are required\n",
    "cap_world = [doc[1:] for doc in text if 'capital-world' in doc[0]][0]\n",
    "cur = [doc[1:] for doc in text if 'currency' in doc[0]][0]\n",
    "city_state = [doc[1:] for doc in text if 'city-in-state' in doc[0]][0]\n",
    "fam = [doc[1:] for doc in text if 'family' in doc[0]][0]\n",
    "adj_adv = [doc[1:] for doc in text if 'gram1-adjective-to-adverb' in doc[0]][0]\n",
    "opp = [doc[1:] for doc in text if 'gram2-opposite' in doc[0]][0]\n",
    "comp = [doc[1:] for doc in text if 'gram3-comparative' in doc[0]][0]\n",
    "nat_adj = [doc[1:] for doc in text if 'gram6-nationality-adjective' in doc[0]][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Glove\n",
    "Source: https://nlp.stanford.edu/projects/glove/<br>\n",
    "Paper: https://nlp.stanford.edu/pubs/glove.pdf<br>\n",
    "Download: http://nlp.stanford.edu/data/glove.6B.zip<br>\n",
    "\n",
    "Text File Used: glove.6B.50d.txt<br>\n",
    "The file contains 6 Billion tokens with 50 dimensions sourced using Wikipedia 2014 and Gigaword 5 datasets.<br>\n",
    "The file is placed in the same directory as the notebook.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing relevant libraries\n",
    "from gensim.test.utils import datapath, get_tmpfile\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to preprocess text in the analogy tests\n",
    "def preprocess(data):\n",
    "    \n",
    "    #Lowercasing all the words and removing '\\t' symbols\n",
    "    data = [doc.lower().replace('\\t', '') for doc in data if doc != '']\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to calculate the result for each analogy test\n",
    "def calculate_result(t, model):\n",
    "    results = []\n",
    "    for analogy in t:\n",
    "        \n",
    "        #Splitting the analogy test into the four component word\n",
    "        a, b, c, d = analogy.split()\n",
    "        \n",
    "        try:\n",
    "            pred = model.most_similar(positive=[b, c], negative=[a], topn = 1)[0][0]\n",
    "            \n",
    "        #In case any of the words in the analogy don't exist in the word vector vocabulary,\n",
    "        #assume that the prediction is a random string, in this case: \"abc\"\n",
    "        except KeyError:\n",
    "            pred = 'abc'\n",
    "            \n",
    "        #The function returns 1 if the model guesses the word correctly, else 0\n",
    "        if pred == d:\n",
    "            results.append(1)        \n",
    "        else:\n",
    "            results.append(0)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using gensim to convert the glove vector text file into word2vec format\n",
    "from gensim.test.utils import datapath, get_tmpfile\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "\n",
    "#Source: https://radimrehurek.com/gensim/scripts/glove2word2vec.html\n",
    "glove_file = datapath(os.path.abspath('glove.6B.50d.txt'))\n",
    "tmp_file = get_tmpfile(os.path.abspath(\"test_word2vec.txt\"))\n",
    "converted_file = glove2word2vec(glove_file, tmp_file) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading the Glove embeddings in word2vec format\n",
    "glove_model = KeyedVectors.load_word2vec_format(os.path.abspath(\"test_word2vec.txt\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Glove model:\n",
      "The accuracy of the model on the 'Capital-World' task:  68.47922192749779\n",
      "The accuracy of the model on the 'Currency' task:  8.314087759815243\n",
      "The accuracy of the model on the 'City-State' task:  15.322253749493312\n",
      "The accuracy of the model on the 'Family' task:  68.97233201581028\n",
      "The accuracy of the model on the 'Adjective-Adverb' task:  15.221774193548388\n",
      "The accuracy of the model on the 'Opposites' task:  9.482758620689655\n",
      "The accuracy of the model on the 'Comparitives' task:  51.80180180180181\n",
      "The accuracy of the model on the 'Nationality-Adjective' task:  85.99124452782989\n"
     ]
    }
   ],
   "source": [
    "#Average execution time is 1 minute for each task\n",
    "print(\"Glove model:\")\n",
    "\n",
    "#Capital World Task\n",
    "cap_world = preprocess(cap_world)\n",
    "y_act = [1]*len(cap_world)\n",
    "y_pred = calculate_result(cap_world, glove_model)\n",
    "print(\"The accuracy of the model on the 'Capital-World' task: \", calc_accuracy(y_pred, y_act)*100)\n",
    "\n",
    "#Currency Task\n",
    "cur = preprocess(cur)\n",
    "y_act = [1]*len(cur)\n",
    "y_pred = calculate_result(cur, glove_model)\n",
    "print(\"The accuracy of the model on the 'Currency' task: \", calc_accuracy(y_pred, y_act)*100)\n",
    "\n",
    "#City State Task\n",
    "city_state = preprocess(city_state)\n",
    "y_act = [1]*len(city_state)\n",
    "y_pred = calculate_result(city_state, glove_model)\n",
    "print(\"The accuracy of the model on the 'City-State' task: \", calc_accuracy(y_pred, y_act)*100)\n",
    "\n",
    "#Family Task\n",
    "fam = preprocess(fam)\n",
    "y_act = [1]*len(fam)\n",
    "y_pred = calculate_result(fam, glove_model)\n",
    "print(\"The accuracy of the model on the 'Family' task: \", calc_accuracy(y_pred, y_act)*100)\n",
    "\n",
    "#Adjective adverb task\n",
    "adj_adv = preprocess(adj_adv)\n",
    "y_act = [1]*len(adj_adv)\n",
    "y_pred = calculate_result(adj_adv, glove_model)\n",
    "print(\"The accuracy of the model on the 'Adjective-Adverb' task: \", calc_accuracy(y_pred, y_act)*100)\n",
    "\n",
    "#Opposites task\n",
    "opp = preprocess(opp)\n",
    "y_act = [1]*len(opp)\n",
    "y_pred = calculate_result(opp, glove_model)\n",
    "print(\"The accuracy of the model on the 'Opposites' task: \", calc_accuracy(y_pred, y_act)*100)\n",
    "\n",
    "#Comparitives task\n",
    "comp = preprocess(comp)\n",
    "y_act = [1]*len(comp)\n",
    "y_pred = calculate_result(comp, glove_model)\n",
    "print(\"The accuracy of the model on the 'Comparitives' task: \", calc_accuracy(y_pred, y_act)*100)\n",
    "\n",
    "#Nationality-Adjective task\n",
    "nat_adj = preprocess(nat_adj)\n",
    "y_act = [1]*len(nat_adj)\n",
    "y_pred = calculate_result(nat_adj, glove_model)\n",
    "print(\"The accuracy of the model on the 'Nationality-Adjective' task: \", calc_accuracy(y_pred, y_act)*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above model fares very poorly in the \"Currency\", \"City-in-State\", \"Adjective-Adverb\" and \"Opposite\" tasks. This may be due to lack of presence in enough contexts in the dataset that the Glove embeddings were sourced from."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LexVec\n",
    "Source: https://github.com/alexandres/lexvec<br>\n",
    "Download Word Embeddings: https://www.dropbox.com/s/kguufyc2xcdi8yk/lexvec.enwiki%2Bnewscrawl.300d.W.pos.vectors.gz?dl=1<br>\n",
    "<br>\n",
    "File Used: lexvec.enwiki+newscrawl.300d.W.pos.vectors<br>\n",
    "The file contains 7 Billion Tokens with 300 dimensions sourced using the Wikipedia 2015 data set and NewsCrawl<br>\n",
    "The file is placed in the same directory as that of the notebook.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading the LexVec embeddings in word2vec format\n",
    "lexvec = KeyedVectors.load_word2vec_format('lexvec.enwiki+newscrawl.300d.W.pos.vectors', binary = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LexVec model:\n",
      "The accuracy of the model on the 'Capital-World' task:  94.36339522546419\n",
      "The accuracy of the model on the 'Currency' task:  22.0554272517321\n",
      "The accuracy of the model on the 'City-State' task:  72.63883259019052\n",
      "The accuracy of the model on the 'Family' task:  87.74703557312253\n",
      "The accuracy of the model on the 'Adjective-Adverb' task:  24.899193548387096\n",
      "The accuracy of the model on the 'Opposites' task:  36.57635467980296\n",
      "The accuracy of the model on the 'Comparitives' task:  87.31231231231232\n",
      "The accuracy of the model on the 'Nationality-Adjective' task:  91.80737961225766\n"
     ]
    }
   ],
   "source": [
    "#Average execution time is 2 minute for each task\n",
    "print(\"LexVec model:\")\n",
    "\n",
    "#Capital World Task\n",
    "cap_world = preprocess(cap_world)\n",
    "y_act = [1]*len(cap_world)\n",
    "y_pred = calculate_result(cap_world, lexvec)\n",
    "print(\"The accuracy of the model on the 'Capital-World' task: \", calc_accuracy(y_pred, y_act)*100)\n",
    "\n",
    "#Currency Task\n",
    "cur = preprocess(cur)\n",
    "y_act = [1]*len(cur)\n",
    "y_pred = calculate_result(cur, lexvec)\n",
    "print(\"The accuracy of the model on the 'Currency' task: \", calc_accuracy(y_pred, y_act)*100)\n",
    "\n",
    "#City State Task\n",
    "city_state = preprocess(city_state)\n",
    "y_act = [1]*len(city_state)\n",
    "y_pred = calculate_result(city_state, lexvec)\n",
    "print(\"The accuracy of the model on the 'City-State' task: \", calc_accuracy(y_pred, y_act)*100)\n",
    "\n",
    "#Family Task\n",
    "fam = preprocess(fam)\n",
    "y_act = [1]*len(fam)\n",
    "y_pred = calculate_result(fam, lexvec)\n",
    "print(\"The accuracy of the model on the 'Family' task: \", calc_accuracy(y_pred, y_act)*100)\n",
    "\n",
    "#Adjective adverb task\n",
    "adj_adv = preprocess(adj_adv)\n",
    "y_act = [1]*len(adj_adv)\n",
    "y_pred = calculate_result(adj_adv, lexvec)\n",
    "print(\"The accuracy of the model on the 'Adjective-Adverb' task: \", calc_accuracy(y_pred, y_act)*100)\n",
    "\n",
    "#Opposites task\n",
    "opp = preprocess(opp)\n",
    "y_act = [1]*len(opp)\n",
    "y_pred = calculate_result(opp, lexvec)\n",
    "print(\"The accuracy of the model on the 'Opposites' task: \", calc_accuracy(y_pred, y_act)*100)\n",
    "\n",
    "#Comparitives task\n",
    "comp = preprocess(comp)\n",
    "y_act = [1]*len(comp)\n",
    "y_pred = calculate_result(comp, lexvec)\n",
    "print(\"The accuracy of the model on the 'Comparitives' task: \", calc_accuracy(y_pred, y_act)*100)\n",
    "\n",
    "#Nationality-Adjective task\n",
    "nat_adj = preprocess(nat_adj)\n",
    "y_act = [1]*len(nat_adj)\n",
    "y_pred = calculate_result(nat_adj, lexvec)\n",
    "print(\"The accuracy of the model on the 'Nationality-Adjective' task: \", calc_accuracy(y_pred, y_act)*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above model has considerably better accuracy than the previous Glove model. This may be due to the higher number of tokens and dimensions in the LexVec Model. The Model also takes a larger amount of time to perform each task. However, the above model still performs poorly on the \"Currency\", \"City-in-State\", \"Adjective-Adverb\" and \"Opposite\" tasks. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Experiment with Word2Vec\n",
    "\n",
    "# w2v_model = KeyedVectors.load_word2vec_format(os.path.abspath(\"GoogleNews-vectors-negative300.bin\"), binary=True0)\n",
    "# y_pred = calculate_result(cap_world, w2v_model)\n",
    "# calc_accuracy(y_pred, y_act)\n",
    "\n",
    "#Accuracy: 0.021441202475685234"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Experiment with FastText\n",
    "\n",
    "# from gensim.models.wrappers import FastText\n",
    "# fasttext_model = KeyedVectors.load_word2vec_format('wiki-news-300d-1M.vec')\n",
    "# y_pred = calculate_result(cap_world, fasttext_model)\n",
    "# calc_accuracy(y_pred, y_act)\n",
    "\n",
    "#Accuracy: 0.16069849690539345"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One known problem with word embeddings is that\n",
    "antonyms (words with meanings considered to be opposites) often have similar\n",
    "embeddings. You can verify this by searching for the top 10 most similar words\n",
    "to a few verbs like increase or enter that have clear antonyms (e.g., decrease\n",
    "and exit, respectively) using the cosine similarity. Discuss why embeddings\n",
    "might have this tendency."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finding the top 10 most similar words using both LexVec and Glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('decrease', 0.85441654920578),\n",
       " ('increased', 0.8349589109420776),\n",
       " ('increases', 0.7846347093582153),\n",
       " ('increasing', 0.7320786714553833),\n",
       " ('decreased', 0.6769896745681763),\n",
       " ('reduce', 0.6768884658813477),\n",
       " ('rise', 0.6671467423439026),\n",
       " ('reduced', 0.6613093614578247),\n",
       " ('decline', 0.6586759090423584),\n",
       " ('reduction', 0.6572977304458618)]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lexvec.most_similar('increase')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('exits', 0.7411397695541382),\n",
       " ('westbound', 0.6021254062652588),\n",
       " ('southbound', 0.6006765365600586),\n",
       " ('eastbound', 0.5971554517745972),\n",
       " ('northbound', 0.5956331491470337),\n",
       " ('exiting', 0.5949065685272217),\n",
       " ('entrance', 0.5674291849136353),\n",
       " ('interchange', 0.5511788725852966),\n",
       " ('ramp', 0.5424832105636597),\n",
       " ('cloverleaf', 0.5038855075836182)]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lexvec.most_similar('exit')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('exits', 0.7869449257850647),\n",
       " ('narrow', 0.6713175773620605),\n",
       " ('route', 0.6655716896057129),\n",
       " ('track', 0.6644213795661926),\n",
       " ('reaching', 0.6516510248184204),\n",
       " ('reach', 0.6463636755943298),\n",
       " ('final', 0.6396181583404541),\n",
       " ('passage', 0.6381533145904541),\n",
       " ('next', 0.6355715394020081),\n",
       " ('northbound', 0.6345898509025574)]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove_model.most_similar('exit')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('down', 0.9523451924324036),\n",
       " ('out', 0.9315087795257568),\n",
       " ('while', 0.9285621047019958),\n",
       " ('back', 0.9047043919563293),\n",
       " ('off', 0.8998678922653198),\n",
       " ('put', 0.8958144187927246),\n",
       " ('just', 0.8927414417266846),\n",
       " ('away', 0.8920525908470154),\n",
       " ('over', 0.8814347386360168),\n",
       " ('to', 0.8726335763931274)]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove_model.most_similar('up')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('refusing', 0.883618950843811),\n",
       " ('insist', 0.8524091839790344),\n",
       " ('accept', 0.8361102938652039),\n",
       " ('intend', 0.822323203086853),\n",
       " ('obliged', 0.8216915130615234),\n",
       " ('cannot', 0.8049349188804626),\n",
       " ('must', 0.8007821440696716),\n",
       " ('ask', 0.7897180318832397),\n",
       " ('willing', 0.7879184484481812),\n",
       " ('agree', 0.7838754653930664)]"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove_model.most_similar('refuse')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above examples, we can clearly see that antonyms tend to have similar word vectors. This is because verbs that are antonyms of each other oftern occur in the same contexts and often perform the same role in sentences. For example, \"Going up the stairs\" and \"Going down the stairs\", \"up\" and \"down\" have similar functions and occur in the same context (traversing the stairs). Word embeddings such as Word2Vec, Glove and LexVec are trained on contextual similarity and so do not actually contain any information about their polarity with respect to each other."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Design two new types of analogy tests that are not part\n",
    "of Mikolov’s analogy dataset. You will create your own test questions (3\n",
    "questions for each type, so in total 6 new questions). Report how well the two\n",
    "sets of embeddings perform on your test questions. You’re encouraged to be\n",
    "adversarial so that the embeddings might get an accuracy of zero! Discuss any\n",
    "interesting observations you have made in the process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Field-Profession <br>\n",
    "music : musician :: science : scientist<br>\n",
    "medicine : doctor :: art : artist<br>\n",
    "comedy : comedian :: war : warrior<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a list of Field - Professional analogy questions\n",
    "professions = ['music musician science scientist',\n",
    "'medicine doctor art artist',\n",
    "'comedy comedian war warrior']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Glove Model\n",
      "The accuracy of the model on the 'Field-Profession' task:  66.66666666666666\n"
     ]
    }
   ],
   "source": [
    "#Using the Glove model\n",
    "professions = preprocess(professions)\n",
    "y_act = [1]*len(professions)\n",
    "y_pred = calculate_result(professions, glove_model)\n",
    "print(\"Glove Model\")\n",
    "print(\"The accuracy of the model on the 'Field-Profession' task: \", calc_accuracy(y_pred, y_act)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LexVec Model\n",
      "The accuracy of the model on the 'Field-Profession' task:  33.33333333333333\n"
     ]
    }
   ],
   "source": [
    "#Using the LexVec Model\n",
    "professions = preprocess(professions)\n",
    "y_act = [1]*len(professions)\n",
    "y_pred = calculate_result(professions, lexvec)\n",
    "print(\"LexVec Model\")\n",
    "print(\"The accuracy of the model on the 'Field-Profession' task: \", calc_accuracy(y_pred, y_act)*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Male Animal - Female Animal<br>\n",
    "peacock : peahen :: stallion : mare<br>\n",
    "billy : nanny :: lion : lioness<br>\n",
    "bull : cow :: boar : sow<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a list of Male - Female analogy question\n",
    "male_female = ['peacock peahen stallion mare',\n",
    "              'billy nanny lion lioness',\n",
    "             'bull cow boar sow',]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Glove Model\n",
      "The accuracy of the model on the 'Male Animal - Female Animal' task:  0.0\n"
     ]
    }
   ],
   "source": [
    "#Using the Glove model\n",
    "male_female = preprocess(male_female)\n",
    "y_act = [1]*len(male_female)\n",
    "y_pred = calculate_result(male_female, glove_model)\n",
    "print(\"Glove Model\")\n",
    "print(\"The accuracy of the model on the 'Male Animal - Female Animal' task: \", calc_accuracy(y_pred, y_act)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LexVec Model\n",
      "The accuracy of the model on the 'Male Animal - Female Animal' task:  0.0\n"
     ]
    }
   ],
   "source": [
    "#Using the LexVec model\n",
    "male_female = preprocess(male_female)\n",
    "y_act = [1]*len(male_female)\n",
    "y_pred = calculate_result(male_female, lexvec)\n",
    "print(\"LexVec Model\")\n",
    "print(\"The accuracy of the model on the 'Male Animal - Female Animal' task: \", calc_accuracy(y_pred, y_act)*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above results show us that the models that we created are not very good at capturing the actual meanings of words. The vectors rely entirely on the context of the word and how and where the word occur in the training corpus for the word embeddings. Our models are able to make some correct predictions in the first analogy \"Field-Profession\" due to how often the words occur in everyday text. The models are trained on everyday text and thus produce results representative of it.<br>\n",
    "The models fail completely on the second analogy task. The words in the analogy task are quite rare and might not occur in the same context. Therefore, the models have an accuracy of 0.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "del glove_model\n",
    "del lexvec"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
