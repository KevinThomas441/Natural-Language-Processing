{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 124
    },
    "colab_type": "code",
    "id": "-ng1DsDokHJV",
    "outputId": "08e6989e-f9fe-425e-b5d4-ce1390181902"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
      "\n",
      "Enter your authorization code:\n",
      "··········\n",
      "Mounted at /content/drive/\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Mrjf1sjnwzXX"
   },
   "source": [
    "Please place tweets_corpus.txt in the same location as the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 121
    },
    "colab_type": "code",
    "id": "w2Iz092dt3w9",
    "outputId": "d933d000-b797-4128-b59a-72f2c7266dfd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download('wordnet')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "lemmatizer = WordNetLemmatizer()\n",
    "import string\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "22pAcY0gZxL5"
   },
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CcRsw3HzkPdT"
   },
   "outputs": [],
   "source": [
    "f = open(\"tweets_corpus.txt\")\n",
    "corpus = f.read()\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "63-b1pA-q5zn"
   },
   "outputs": [],
   "source": [
    "corpus = corpus.split(\"\\n\")\n",
    "corpus = corpus[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 452
    },
    "colab_type": "code",
    "id": "3fC0yNRMYuhb",
    "outputId": "79ba7bac-707e-4ba8-a8da-bb4becaa4373"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"81499877556760576\\tBandaging up my paper-cuts , having cheesecake for dinner , and calling it a night . We're doin it big here in NYC .\",\n",
       " \"81500716438523904\\tI haven't had any krispy kremes or strawberry trifles since I started gym * cries *\",\n",
       " '81503002321616896\\tBacon/cheddar slider topped w/fried egg & Blue cheese slider topped w/avocado & purple cherokee tomato',\n",
       " '81507775422791680\\tNacho w/ cheese on my shirt ! Uggghhh',\n",
       " '81534165975171072\\tyou aint nuffin but a piece of cheese without the corners .. in other words , you will never be a slice . BITCH .',\n",
       " '81535634019323904\\tTAG_USERNAME TAG_USERNAME Mmmm ... cheese ... dreaming of a squirrel burger with cheese !',\n",
       " '81577509950459904\\tMmmm cheese',\n",
       " \"81582996083326976\\tTAG_USERNAME 1st off I'm like 1 year younger than u , 2nd age is just a number , 3rd ima cater ur wedding wit patty n cheese\",\n",
       " '81587643376336896\\tRT TAG_USERNAME : I want a steak and cheese egg roll right now .',\n",
       " '81600113016971264\\tthink imma eat some cheesecake befor i lay down ... Havent had it in a while',\n",
       " '81623945064890368\\tA mixed one mostly strawberry peach little white cherry dr pepper or coke etc',\n",
       " '81644157432643584\\tMy stomach was yelling at me telling me to get my ass up nd get somethin to eat . So I went nd got cheesesticks nd a waterbottle .',\n",
       " '81656304107651072\\tchocolate mint , cookies & cream , very berry strawberry , and chocolate caramel~ all blend perfectly in my mouth',\n",
       " \"81673244926685184\\tI think I want some cheese eggs and pancakes ... but will I cook ? Where's my gf . This ain't right to make such a hard decision\",\n",
       " \"81715158593966080\\tTAG_USERNAME Totally . It's also good on fish , chicken , veggies . Oh , and desserts . Drizzling it over strawberry tarts for a party tonight ! -C\",\n",
       " '81716618236928000\\tTAG_USERNAME I want to get the strawberry cheesecake and candy bear . When does the 2 for $ 38 expire ?',\n",
       " '81736742478155778\\tMade myself some scrambled eggs with cheese and bacon bits',\n",
       " '81842384404623360\\tTAG_USERNAME you could try it but its not great haha if i could recommend , get the strawberry one thats ma fave !',\n",
       " '81844590625304576\\tTAG_USERNAME then im 100% pure strawberry flavoured hmmm tasty',\n",
       " '82461950231064576\\tjust had the most beautiful pink coloured raspberry , strawberry & banana smoothie',\n",
       " '82650970722533376\\tI went swimming , then ate asparagus bacon egg cheese biscuit goodness , then watched Date Night . It was ... it was good . TAG_FINAL_HASHTAGS',\n",
       " '85032815321825280\\tCheese hashbrowns , turkey bacon , veggie tofu scramble , rice , french toast , scrambled eggs , strawberries & cantaloupe . TAG_FINAL_HASHTAGS',\n",
       " '85094773555335168\\tTAG_HASHTAGS using cream cheese icing on chocolate cake .. just use vanilla or strawberry',\n",
       " \"86441828815089664\\tRT TAG_USERNAME : RT TAG_USERNAME : Pancakes , bacon , eggs w/ cheese , & hashbrown casserole on deck TAG_HASHTAGS < ~i want sum !!!!! Ok it's good too\"]"
      ]
     },
     "execution_count": 6,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5KedJRmO8dsb"
   },
   "source": [
    "## Problem 1\n",
    "We  have  given  you  a  small  set  of  data  in  the  form  of tweets. Each line in the file begins with a document ID, followed by the text of the tweet. Implement a function to create aninverted index of these documents. You  may  use  the  approach  described  in  the  Manning  book(link  below),  or  you may come up with your own algorithm.<br>\n",
    "https://nlp.stanford.edu/IR-book/html/htmledition/a-first-take-at-building-an-inverted-index-1.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CWKD7_q7ucgj"
   },
   "outputs": [],
   "source": [
    "#Splitting the data into two lists containing document_IDs and tweets\n",
    "doc_ids = []\n",
    "tweets = []\n",
    "documents = defaultdict(list)\n",
    "for text in corpus:\n",
    "    id = re.findall(\"^(\\d{17})\\t\", text)[0]\n",
    "    tweet = re.findall(\"\\d{17}\\t(.*)$\", text)[0]\n",
    "    doc_ids.append(id)\n",
    "    tweets.append(tweet)\n",
    "    documents[id] = tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 469
    },
    "colab_type": "code",
    "id": "ijCfD2Qs6sqw",
    "outputId": "e48307c6-9586-4df3-b29c-9c68c03acc67"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(list,\n",
       "            {'81499877556760576': \"Bandaging up my paper-cuts , having cheesecake for dinner , and calling it a night . We're doin it big here in NYC .\",\n",
       "             '81500716438523904': \"I haven't had any krispy kremes or strawberry trifles since I started gym * cries *\",\n",
       "             '81503002321616896': 'Bacon/cheddar slider topped w/fried egg & Blue cheese slider topped w/avocado & purple cherokee tomato',\n",
       "             '81507775422791680': 'Nacho w/ cheese on my shirt ! Uggghhh',\n",
       "             '81534165975171072': 'you aint nuffin but a piece of cheese without the corners .. in other words , you will never be a slice . BITCH .',\n",
       "             '81535634019323904': 'TAG_USERNAME TAG_USERNAME Mmmm ... cheese ... dreaming of a squirrel burger with cheese !',\n",
       "             '81577509950459904': 'Mmmm cheese',\n",
       "             '81582996083326976': \"TAG_USERNAME 1st off I'm like 1 year younger than u , 2nd age is just a number , 3rd ima cater ur wedding wit patty n cheese\",\n",
       "             '81587643376336896': 'RT TAG_USERNAME : I want a steak and cheese egg roll right now .',\n",
       "             '81600113016971264': 'think imma eat some cheesecake befor i lay down ... Havent had it in a while',\n",
       "             '81623945064890368': 'A mixed one mostly strawberry peach little white cherry dr pepper or coke etc',\n",
       "             '81644157432643584': 'My stomach was yelling at me telling me to get my ass up nd get somethin to eat . So I went nd got cheesesticks nd a waterbottle .',\n",
       "             '81656304107651072': 'chocolate mint , cookies & cream , very berry strawberry , and chocolate caramel~ all blend perfectly in my mouth',\n",
       "             '81673244926685184': \"I think I want some cheese eggs and pancakes ... but will I cook ? Where's my gf . This ain't right to make such a hard decision\",\n",
       "             '81715158593966080': \"TAG_USERNAME Totally . It's also good on fish , chicken , veggies . Oh , and desserts . Drizzling it over strawberry tarts for a party tonight ! -C\",\n",
       "             '81716618236928000': 'TAG_USERNAME I want to get the strawberry cheesecake and candy bear . When does the 2 for $ 38 expire ?',\n",
       "             '81736742478155778': 'Made myself some scrambled eggs with cheese and bacon bits',\n",
       "             '81842384404623360': 'TAG_USERNAME you could try it but its not great haha if i could recommend , get the strawberry one thats ma fave !',\n",
       "             '81844590625304576': 'TAG_USERNAME then im 100% pure strawberry flavoured hmmm tasty',\n",
       "             '82461950231064576': 'just had the most beautiful pink coloured raspberry , strawberry & banana smoothie',\n",
       "             '82650970722533376': 'I went swimming , then ate asparagus bacon egg cheese biscuit goodness , then watched Date Night . It was ... it was good . TAG_FINAL_HASHTAGS',\n",
       "             '85032815321825280': 'Cheese hashbrowns , turkey bacon , veggie tofu scramble , rice , french toast , scrambled eggs , strawberries & cantaloupe . TAG_FINAL_HASHTAGS',\n",
       "             '85094773555335168': 'TAG_HASHTAGS using cream cheese icing on chocolate cake .. just use vanilla or strawberry',\n",
       "             '86441828815089664': \"RT TAG_USERNAME : RT TAG_USERNAME : Pancakes , bacon , eggs w/ cheese , & hashbrown casserole on deck TAG_HASHTAGS < ~i want sum !!!!! Ok it's good too\"})"
      ]
     },
     "execution_count": 55,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7ZSUSxjJ3Yva"
   },
   "source": [
    "### Preprocess Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rHqjfT3D0K74"
   },
   "outputs": [],
   "source": [
    "#Function to preprocess text\n",
    "def preprocess(text):\n",
    "  #Lowercasing all the text\n",
    "  text = text.lower()\n",
    "\n",
    "  #Removing punctuation\n",
    "  text = \" \".join(re.sub(\"[!#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~]\", \" \", text).split())\n",
    "\n",
    "  #Tokenizing the string\n",
    "  text = word_tokenize(text)\n",
    "\n",
    "  #Removing stop words and words shorter than two characters\n",
    "  text = [lemmatizer.lemmatize(word) for word in text if word not in stop_words and len(word)>2]\n",
    "\n",
    "  #Returning document ID and prcocessed tweets\n",
    "  return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Mu8QWq423byV"
   },
   "source": [
    "### Creating Inverted Indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DB5klp0-weD5"
   },
   "outputs": [],
   "source": [
    "#Creating an inverted index as a defaultdict\n",
    "from collections import defaultdict\n",
    "\n",
    "#Initializing the default dict\n",
    "inv_indx = defaultdict(list)\n",
    "\n",
    "#Iterating over the tweets in the corpus\n",
    "for idx, text in enumerate(tweets):\n",
    "    tweet = preprocess(text)\n",
    "\n",
    "    #Iterating over each word in the tweet\n",
    "    for word in tweet:\n",
    "        inv_indx[word].append(doc_ids[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Yny0rQAM-3E5"
   },
   "outputs": [],
   "source": [
    "inv_indx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SSSL_qOs8pGz"
   },
   "source": [
    "## Problem 2\n",
    "Write  a  function  to  implement  the  merge  algorithm  as discussed  in  class. Your  codeshould  allowintersecting  the  postings  of  two terms, as well asprocess simple Boolean queries. When there are multiple query terms,  make  sure  that  your  algorithm  uses  the  optimization  described  in Manningbook of performing the most restrictive intersection first.<br>\n",
    "https://nlp.stanford.edu/IR-book/html/htmledition/processing-boolean-queries-1.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "frvlexTu3fvg"
   },
   "source": [
    "### Boolean Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UjxIaEvmDo-8"
   },
   "outputs": [],
   "source": [
    "#Function to AND to terms\n",
    "def AND(p1, p2):\n",
    "    i=j=0\n",
    "    ans = []\n",
    "    while i<len(p1) and j<len(p2):\n",
    "        if p1[i]==p2[j]:\n",
    "            ans.append(p1[i])\n",
    "            i+=1\n",
    "            j+=1\n",
    "        else:\n",
    "            if p1[i]<p2[j]:\n",
    "                i+=1\n",
    "            else:\n",
    "                j+=1\n",
    "    return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "I8Fu9YoqGnbx"
   },
   "outputs": [],
   "source": [
    "#Function to OR two terms\n",
    "def OR(p1, p2):\n",
    "    ans = []\n",
    "    i=j=0\n",
    "    while i < len(p1) and j < len(p2):\n",
    "        if p1[i] == p2[j]:\n",
    "            ans.append(p1[i])\n",
    "            i += 1\n",
    "            j += 1\n",
    "        else:\n",
    "            if p1[i] > p2[j]:\n",
    "                ans.append(p2[j])\n",
    "                j += 1\n",
    "            else:\n",
    "                ans.append(p1[i])\n",
    "                i += 1\n",
    "    while i < len(p1):\n",
    "        ans.append(p1[i])\n",
    "        i += 1\n",
    "    while j < len(p2):\n",
    "        ans.append(p2[j])\n",
    "        j += 1\n",
    "    return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Y4AebBVPKQd0"
   },
   "outputs": [],
   "source": [
    "#Function to preprocess queries\n",
    "def preprocess_query(text):\n",
    "  #Lowercasing all the text\n",
    "  text = text.lower()\n",
    "\n",
    "  #Removing punctuation\n",
    "  text = \" \".join(re.sub(\"[!#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~]\", \" \", text).split())\n",
    "\n",
    "  #Tokenizing the string\n",
    "  text = word_tokenize(text)\n",
    "\n",
    "  #Removing stop words and words shorter than two characters\n",
    "  text = [lemmatizer.lemmatize(word) for word in text]\n",
    "\n",
    "  #Returning document ID and prcocessed tweets\n",
    "  return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8xBRL7lQ3jkG"
   },
   "source": [
    "### Query Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_Zwwc-C2fzNj"
   },
   "outputs": [],
   "source": [
    "#Function to perform Boolean Search\n",
    "def boolean_search(query):\n",
    "    query = preprocess_query(query)\n",
    "    terms = []\n",
    "    operations = []\n",
    "    for w in query:\n",
    "        if w == \"and\" or w==\"or\":\n",
    "            operations.append(w)\n",
    "        else:\n",
    "            terms.append(w)\n",
    "\n",
    "    # for two query terms\n",
    "    ans = []\n",
    "    if len(query)<=3:\n",
    "    p1 = query[0]\n",
    "    p2 = query[2]\n",
    "    if query[1]==\"and\":\n",
    "        ans = AND(inv_indx[lemmatizer.lemmatize(p1)], inv_indx[lemmatizer.lemmatize(p2)])\n",
    "    elif query[1]==\"or\":\n",
    "        ans = OR(inv_indx[lemmatizer.lemmatize(p1)], inv_indx[lemmatizer.lemmatize(p2)])\n",
    "    else:\n",
    "        print(\"Invalid Query\")\n",
    "        print(\"Please use only AND and OR queries\")\n",
    "\n",
    "  # for more than two queries\n",
    "  else:\n",
    "    i = 1\n",
    "    j = 0\n",
    "    terms = [lemmatizer.lemmatize(x) for x in terms]\n",
    "    posting_len = [len(x) for x in terms]\n",
    "    p1 = inv_indx[lemmatizer.lemmatize(terms[0])]\n",
    "    while i<len(terms):\n",
    "        p2 = inv_indx[lemmatizer.lemmatize(terms[i])]\n",
    "        if operations[j]==\"and\":\n",
    "            p1 = AND(p1, p2)\n",
    "        elif operations[j]==\"or\":\n",
    "            p1 = OR(p1, p2)\n",
    "            i+=1\n",
    "            j+=1\n",
    "        ans = p1\n",
    "    return terms, ans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Uskml6cRz21I"
   },
   "source": [
    "### Testing\n",
    "Please enter your query after running the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 193
    },
    "colab_type": "code",
    "id": "g3jNJjZSgrTF",
    "outputId": "14cc99d1-2c88-4771-c008-ab08d1812419"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please Enter your query:bacon and egg and cheese\n",
      "\n",
      "\n",
      "Results:\n",
      "Bacon/cheddar slider topped w/fried egg & Blue cheese slider topped w/avocado & purple cherokee tomato\n",
      "Made myself some scrambled eggs with cheese and bacon bits\n",
      "I went swimming , then ate asparagus bacon egg cheese biscuit goodness , then watched Date Night . It was ... it was good . TAG_FINAL_HASHTAGS\n",
      "Cheese hashbrowns , turkey bacon , veggie tofu scramble , rice , french toast , scrambled eggs , strawberries & cantaloupe . TAG_FINAL_HASHTAGS\n",
      "RT TAG_USERNAME : RT TAG_USERNAME : Pancakes , bacon , eggs w/ cheese , & hashbrown casserole on deck TAG_HASHTAGS < ~i want sum !!!!! Ok it's good too\n"
     ]
    }
   ],
   "source": [
    "query = input(\"Please Enter your query:\")\n",
    "terms, ans = boolean_search(query)\n",
    "print(\"\")\n",
    "print(\"\")\n",
    "print(\"Results:\")\n",
    "if len(ans)>0:\n",
    "    for a in ans:\n",
    "        print(documents[a])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kWYn3nsJGkKf"
   },
   "source": [
    "## Problem 3\n",
    "Extend the system from Problem 2 to perform simple TF-IDF  scoring  of  the  retrieved  results. There  is no  need to  worry  about  any weight normalizations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eQEB8D1W2CEB"
   },
   "source": [
    "### Calculating Term Frequencies\n",
    "\n",
    "Reference: https://towardsdatascience.com/tf-idf-for-document-ranking-from-scratch-in-python-on-real-world-dataset-796d339a4089"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "p-TwQHxcAiAt"
   },
   "outputs": [],
   "source": [
    "#Calculating Document Frequencies for each word\n",
    "term_frequencies = {}\n",
    "for i in range(len(tweets)):\n",
    "    tokens = preprocess(tweets[i])\n",
    "    for w in tokens:\n",
    "        try:\n",
    "            term_frequencies[w].add(i)\n",
    "        except:\n",
    "            term_frequencies[w] = {i}\n",
    "for tf in term_frequencies:\n",
    "    term_frequencies[tf]=len(term_frequencies[tf])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "p3eYDM9m3FWm"
   },
   "source": [
    "Calculating TF_IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "T4vZmMaZOcgu"
   },
   "outputs": [],
   "source": [
    "#Calculating TF_IDF\n",
    "tf_idf = {}\n",
    "\n",
    "#Iterating over\n",
    "for i in range(len(tweets)):\n",
    "    tokens = preprocess(tweets[i])\n",
    "    counter = Counter(tokens)\n",
    "    words_count = len(tokens)\n",
    "    for token in np.unique(tokens):\n",
    "\n",
    "        #Calculating Term Frequencies\n",
    "        tf = counter[token]/words_count\n",
    "\n",
    "        #Obtaining Document Frequencies\n",
    "        df = term_frequencies[token]\n",
    "\n",
    "        #Calculating Inverse Document Frequencies\n",
    "        idf = np.log(len(tweets)/(df+1))\n",
    "\n",
    "        #Calculating TF-IDF\n",
    "        tf_idf[doc_ids[i], token] = tf*idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "s-hbR5H3cS3-"
   },
   "outputs": [],
   "source": [
    "#Scoring each tweet using the sum of TF-IDF vectors\n",
    "score = defaultdict(list)\n",
    "for a in ans:\n",
    "    score[a] = 0\n",
    "    for word in terms:\n",
    "        score[a]+=tf_idf[a, word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Q54Xgiqqxx9P"
   },
   "outputs": [],
   "source": [
    "import operator\n",
    "\n",
    "#Sorting in descending order\n",
    "sorted_score = sorted(score.items(), key=operator.itemgetter(1), reverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IT3c2eyx1-Ic"
   },
   "source": [
    "### Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 124
    },
    "colab_type": "code",
    "id": "ZC_vqw68yBOo",
    "outputId": "ea415111-d533-4c16-a215-5c942d921c43"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Made myself some scrambled eggs with cheese and bacon bits 0.5039838584201145\n",
      "Bacon/cheddar slider topped w/fried egg & Blue cheese slider topped w/avocado & purple cherokee tomato 0.21599308218004906\n",
      "I went swimming , then ate asparagus bacon egg cheese biscuit goodness , then watched Date Night . It was ... it was good . TAG_FINAL_HASHTAGS 0.18899394690754295\n",
      "RT TAG_USERNAME : RT TAG_USERNAME : Pancakes , bacon , eggs w/ cheese , & hashbrown casserole on deck TAG_HASHTAGS < ~i want sum !!!!! Ok it's good too 0.18899394690754295\n",
      "Cheese hashbrowns , turkey bacon , veggie tofu scramble , rice , french toast , scrambled eggs , strawberries & cantaloupe . TAG_FINAL_HASHTAGS 0.17787665591298157\n"
     ]
    }
   ],
   "source": [
    "#Results\n",
    "for i in sorted_score:\n",
    "    print(documents[i[0]], i[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jwuZ1lcaymsM"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Copy of Kevin_Thomas_Assignment_4.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
